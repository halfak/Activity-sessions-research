This section is intended to both serve as a description of our methodology as well as to instruct readers on how to apply the same methods to their own dataset.  First, we will discuss how we recommend applying our methodology for identifying inter-activity type component clusters to a dataset, before describing the origin of our datasets and the cleanup we performed in order to generate inter-activity times to fit.

\subsection{Fitting inter-activity times}
First, we must gather a dataset of user-initiated actions with timestamps of at least seconds resolution.  We generate inter-activity times on a per-user basis, so a relatively robust user identifier is necessary.  While a persistent user identifier such as one associated with a user account is preferable, we have found that, in the case of request logs, a fingerprint based on the request's IP and User-agent seems to be sufficient.

Once we have per-user inter-activity times, we plot a histogram based on the logarithmically scaled inter-activity time and look for evidence of a valley.  Given the observations we have seen (and report in section~\ref{sec:results_and_discussion}), we expect to see a valley around about 1 hour with peaks around 1 minute and 1 day.  It is at this time that anomalies in the data should be detected and removed.  For example, we found that the time between Wikimedia Mobile Views (described in the next section) had an absurd spike at exactly 18 minutes of inter-activity time caused by a few (likely automated) users and removed their activities from the dataset.

Next, we try to fit a two component gaussian mixture model using expectation maximization~\cite{benaglia2009mixtools} and visually inspect the results\footnote{Note that we tried several strategies for statistically confirming the most appropriate fit -- of which we found Davies--Bouldin index (DBI)~\cite{davies1979cluster} to be most reasonable -- but none were as good as a simple visual inspection, so we employ and recommend the same.}  When the simple bimodal components did not appear to fit the data appropriately, we explored the addition of components to the mixture model with careful skepticism and repeated visual inspection.

Finally, if we have found what appears to be an appropriate fit, we identify a theoretically optimal inter-activity threshold for identifying sessions by finding the point where inter-activity time is equally likely to be within the gaussians fit with sub-hour means (within-session) and gaussians fit with means beyond an hour (between-session).

\subsection{Datasets}
To test this approach to session identification, we used a variety of datasets covering multiple sites, user groups and classes of action.

% FIXME: if time, the sentence listing the number of pageviews per dataset
% is tricky because it uses commas as thousands-separator and for listing.
\leadin{Wikimedia sites.} One of the broadest groups of datasets comes from the Wikimedia websites (such as Wikipedia) and covers both pageviews (read actions) and edits. For the pageviews, we used three datasets, each consisting of randomly-sampled pageview events from the Wikimedia request logs. These covered app views (pageviews from the official mobile app), mobile views (pageviews to the mobile site versions) and desktop views (pageviews to the desktop site versions). 100,000 IP addresses (or UUIDs, in the case of the app, since it has those built in) were selected, and all requests from those IPs/UUIDs for the month of October 2014 were retrieved. Excepting app views, where the existing UUID was used, a UUID was produced by hashing the IP address, the user agent, and the accept\_language provided with each request. After filtering out known crawlers and automata using tobie's ua-parser\footnote{\url{https://github.com/tobie/ua-parser}}, the result was three pageview datasets consisting of 2,376,891, 932,754 and 2,285,521 pageviews, respectively. These came from 100,000, 235,067 or 247,269 UUIDs. We also extracted inter-edit times from the English Wikipedia using the methodology we employed in~\cite{geiger2013using} -- randomly selecting 1 million edits from 157,342 registered users.

\leadin{AOL search} Contrasting with the Wikimedia datasets we used the (now infamous) AOL search logs\footnote{These logs are controversial due to their inclusion of search terms containing private information, and there has historically been an ethical debate about their use. We are confident, however, that our usage does not have ethical implications; the dataset has been modified to strip search terms, and consists solely of unique IDs and timestamps, as has been used in the past.\cite{mehrzadi2012onextracting}  See \url{https://en.wikipedia.org/wiki/AOL_search_data_leak}} (aol, search) consisting of 36,389,567 search actions from 657,427 unique IDs. These actions span from March through May 2006.

\leadin{Cyclopath} We also have a dataset from Cyclopath, a computational geowiki leveraging cyclist knowledge~\cite{priedhorsky2008computational}.  The dataset consists of HTTP requests to the Cyclopath server that consist of a request for a cycle route (cyclopath, route get). This came to 6,123 requests from 2,233 distinct registered users.

\leadin{Movielens} A useful source of datasets is the MovieLens movie recommender system, which has been in use since 1997. As of November 2014 there are 225,543 unique users who have provided more than 21 million movie ratings for more than 25,000 movies. We used two MovieLens datasets, one (movielens, rating) consisting of movie rating actions from between 1997 until 5 November 2014, and the other (movielens, search) consisting of search actions from 19 December 2007 to 1 January 2014.

\leadin{StackOverflow}. This popular question/answer system relating to programming and software engineering regularly releases public data dumps. We focused on questions asked (and the answers provided to those questions) between July 2008 and September 2013. The question dataset (stack overflow, question) consists of 6,397,301 questions from 1,191,748 distinct users, while the answer dataset (stack overflow, answer) consists of 11,463,991 answers from 790,713 distinct users.

% FIXME: cannot use "~" directly as a mathematical operator, changed to wording,
% but that needs checking against the numbers, or maybe use $\sim$?
\leadin{OpenStreetMap (OSM)} This open-source alternative to services such as Google Maps also publishes regular database dumps. We took a full history dump of OSM contributions as of 24 February 2014, restricting this to the North American region as defined by Geofabrik\footnote{\url{http://download.geofabrik.de/north-america.html}}, which consists of the United States, Canada and Greenland. OSM groups individual changes to the map into \textit{changesets}\footnote{\url{http://wiki.openstreetmap.org/wiki/API_v0.6#Changesets_2}} when an editor saves their work. We used the timestamp of the last revision in a changeset as the time of the user action. The resulting dataset (osm, changeset) contained about 13.4 million changesets from 46,595 distinct users.

\leadin{League of Legends} This widely-played online multiplayer game supports an extension that adds a rating system for users and logs games and play times for the wide set of users of the extension.  Notably, we used this dataset in previous work to study the effect of deviant behaviour on player retention~\cite{shores2014identification}. We took this dataset - consisting of roughly 2.5 million unique players participating in almost 166m games - and looked at the intertime values between finishing a game and playing the next one (lol, game (between)). Though not all games were captured (see~\cite{shores2014identification} for more details), missing data is believed to be most prevalent around newer players with less consistent play habits.

Taken together, these datasets represent seven different systems, include different browsing mechanisms (apps, mobile devices, desktop devices), and different classes of interaction (web search \& route finding, contributions to collaboratively edited artifacts, page reads, and games played).
